{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/singhayushh/EC881--Assignment/blob/linux/DiabeticRetinopathy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Gj_hEf85TaBn"
      },
      "source": [
        "## Overview\n",
        "\n",
        "The goal is to make a highly accurate diabetic retinopathy model by using different CNN architectures (viz. MobileNet, EfficientNet, Inception V3 and ResNet) for a comparative study and push the best working model to deployment for live usage of the AI for actual patient images in various medicinal institutes.\n",
        "\n",
        "The model can be massively improved with:\n",
        "\n",
        "- high-resolution images\n",
        "- better data sampling\n",
        "- ensuring there is no leaking between training and validation sets.\n",
        "- better target variable (age) normalization\n",
        "- pretrained models\n",
        "- attention/related techniques to focus on areas\n",
        "\n",
        "************\n",
        "\n",
        "### Authors\n",
        "\n",
        "- [Ayush Singh](https://github.com/singhayushh)\n",
        "- [Agni Sain](https://linkedin.com/in/)\n",
        "- [Mayukh Sen](https://linkedin.com/in/)\n",
        "- [Aryan Shaw](https://linkedin.com/in/)\n",
        "\n",
        "************\n",
        "\n",
        "### The Model\n",
        "\n",
        "The model we create will be run through four different CNN architectures:\n",
        "- MobileNet v3\n",
        "- Inception v3\n",
        "- ResNet\n",
        "- EfficientNet\n",
        "- DenseNet\n",
        "\n",
        "All five models will be trained and tested on the same data and based on the output, the most accurate model will be used in the live production environment.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uiY4jKa3ae58"
      },
      "source": [
        "## 1. Download Data\n",
        "\n",
        "#### 1.1. Mount google drive\n",
        "\n",
        "We will be using the test.001 and train.001 images from the Kaggle 2015 dataset, which have been uploaded to google drive for easier access."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-L8g6Pw5aSLl"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vre3kO__cugA"
      },
      "source": [
        "#### 1.2. Unzip dataset\n",
        "\n",
        "In this step, we will unzip the dataset to train and test directories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ojg_xQrQc9AB"
      },
      "outputs": [],
      "source": [
        "# download dataset from kaggle cli\n",
        "!kaggle competitions download -c 'diabetic-retinopathy-detection'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wee2BNfldYyr"
      },
      "outputs": [],
      "source": [
        "# Create train and test directories\n",
        "!mkdir train\n",
        "!mkdir test\n",
        "\n",
        "# Move ZIP files to their directories\n",
        "!mv /content/drive/MyDrive/dataset/test.* test\n",
        "!mv /content/drive/MyDrive/dataset/train* train\n",
        "\n",
        "# Extract data\n",
        "!7za x train/train.zip\n",
        "!7za x test/test.zip\n",
        "\n",
        "!rm train/train.zip\n",
        "!rm test/test.zip"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ws5xCylsIqM"
      },
      "source": [
        "## 2. Image Preprocessing\n",
        "\n",
        "#### 2.1. Crop and Resize\n",
        "\n",
        "All images were scaled down to 256 by 256. Despite taking longer to train, the detail present in photos of this size is much greater then at 128 by 128."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5m5Nzp-Ms15c"
      },
      "outputs": [],
      "source": [
        "# package imports\n",
        "import os\n",
        "import sys\n",
        "from PIL import ImageFile\n",
        "from skimage import io\n",
        "from skimage.transform import resize\n",
        "import numpy as np\n",
        "\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "# utility function to create directory with given name if absent\n",
        "def create_directory(directory):\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "# crop and resize given image and save to new path\n",
        "def crop_and_resize_images(path, new_path, cropx, cropy, img_size=256):\n",
        "    create_directory(new_path)\n",
        "    dirs = [l for l in os.listdir(path) if l != '.DS_Store']\n",
        "    total = 0\n",
        "    for item in dirs:\n",
        "        img = io.imread(path+item)\n",
        "        y,x,channel = img.shape\n",
        "        startx = x//2-(cropx//2)\n",
        "        starty = y//2-(cropy//2)\n",
        "        img = img[starty:starty+cropy,startx:startx+cropx]\n",
        "        img = resize(img, (256,256))\n",
        "        io.imsave(str(new_path + item), img)\n",
        "        total += 1\n",
        "        print(\"Saving: \", item, total)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    crop_and_resize_images(path='/content/train/', new_path='/content/train-resized-256/', cropx=1800, cropy=1800, img_size=256)\n",
        "    crop_and_resize_images(path='/content/test/', new_path='/content/test-resized-256/', cropx=1800, cropy=1800, img_size=256)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hAhbAEPWtoBQ"
      },
      "source": [
        "#### 2.2. Training data pruning\n",
        "\n",
        "Scikit-Image raised multiple warnings during resizing, due to these images having no color space. Because of this, any images that were completely black were removed from the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvmfzlDHy6SE"
      },
      "outputs": [],
      "source": [
        "# package imports\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "# create a 'image' named column with non-black images\n",
        "def find_black_images(file_path, df):\n",
        "    lst_imgs = [l for l in df['image']]\n",
        "    return [1 if np.mean(np.array(Image.open(file_path + img))) == 0 else 0 for img in lst_imgs]\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    start_time = time.time()\n",
        "    trainLabels = pd.read_csv('/content/labels/trainLabels.csv')\n",
        "\n",
        "    trainLabels['image'] = [i + '.jpeg' for i in trainLabels['image']]\n",
        "    trainLabels['black'] = np.nan\n",
        "\n",
        "    trainLabels['black'] = find_black_images('/content/train-resized-256/', trainLabels)\n",
        "    trainLabels = trainLabels.loc[trainLabels['black'] == 0]\n",
        "    trainLabels.to_csv('trainLabels_master.csv', index=False, header=True)\n",
        "\n",
        "    print(\"Completed\")\n",
        "    print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QU18Hojoz7Cg"
      },
      "source": [
        "#### 2.3. Image Rotation\n",
        "\n",
        "In order to reduce noise from the images, all images were rotated and mirrored."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkQtpDRb0SHY"
      },
      "outputs": [],
      "source": [
        "# packages import\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from skimage import io\n",
        "from skimage.transform import rotate\n",
        "from cv2 import cv2\n",
        "import os\n",
        "import time\n",
        "\n",
        "def rotate_images(file_path, degrees_of_rotation, lst_imgs):\n",
        "    for l in lst_imgs:\n",
        "        img = io.imread(file_path + str(l) + '.jpeg')\n",
        "        img = rotate(img, degrees_of_rotation)\n",
        "        io.imsave(file_path + str(l) + '_' + str(degrees_of_rotation) + '.jpeg', img)\n",
        "\n",
        "\n",
        "def mirror_images(file_path, mirror_direction, lst_imgs):\n",
        "    for l in lst_imgs:\n",
        "        img = cv2.imread(file_path + str(l) + '.jpeg')\n",
        "        img = cv2.flip(img, 1)\n",
        "        cv2.imwrite(file_path + str(l) + '_mir' + '.jpeg', img)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    start_time = time.time()\n",
        "    trainLabels = pd.read_csv(\"/content/labels/trainLabels_master.csv\")\n",
        "\n",
        "    trainLabels['image'] = trainLabels['image'].str.rstrip('.jpeg')\n",
        "    trainLabels_no_DR = trainLabels[trainLabels['level'] == 0]\n",
        "    trainLabels_DR = trainLabels[trainLabels['level'] >= 1]\n",
        "\n",
        "    lst_imgs_no_DR = [i for i in trainLabels_no_DR['image']]\n",
        "    lst_imgs_DR = [i for i in trainLabels_DR['image']]\n",
        "\n",
        "    # Mirror Images with no DR one time\n",
        "    print(\"Mirroring Non-DR Images\")\n",
        "    mirror_images('/content/train-resized-256/', 1, lst_imgs_no_DR)\n",
        "\n",
        "\n",
        "    # Rotate all images that have any level of DR\n",
        "    print(\"Rotating 90 Degrees\")\n",
        "    rotate_images('/content/train-resized-256/', 90, lst_imgs_DR)\n",
        "    print(\"Rotating 120 Degrees\")\n",
        "    rotate_images('/content/train-resized-256/', 120, lst_imgs_DR)\n",
        "    print(\"Rotating 180 Degrees\")\n",
        "    rotate_images('/content/train-resized-256/', 180, lst_imgs_DR)\n",
        "    print(\"Rotating 270 Degrees\")\n",
        "    rotate_images('/content/train-resized-256/', 270, lst_imgs_DR)\n",
        "    print(\"Mirroring DR Images\")\n",
        "    mirror_images('/content/train-resized-256/', 0, lst_imgs_DR)\n",
        "    print(\"Completed\")\n",
        "    print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2.4. Reconciling image labels\n",
        "\n",
        "In this step, we read all image files into a list, remove their suffixes and write the resultant list into a new csv."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def get_lst_images(file_path):\n",
        "    return [i for i in os.listdir(file_path) if i != '.DS_Store']\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    trainLabels = pd.read_csv(\"../labels/trainLabels_master.csv\")\n",
        "    lst_imgs = get_lst_images('../data/train-resized-256/')\n",
        "    new_trainLabels = pd.DataFrame({'image': lst_imgs})\n",
        "    new_trainLabels['image2'] = new_trainLabels.image\n",
        "    new_trainLabels['image2'] = new_trainLabels.loc[:, 'image2'].apply(lambda x: '_'.join(x.split('_')[0:2]))\n",
        "    new_trainLabels['image2'] = new_trainLabels.loc[:, 'image2'].apply(\n",
        "        lambda x: '_'.join(x.split('_')[0:2]).strip('.jpeg') + '.jpeg')\n",
        "    new_trainLabels.columns = ['train_image_name', 'image']\n",
        "    trainLabels = pd.merge(trainLabels, new_trainLabels, how='outer', on='image')\n",
        "    trainLabels.drop(['black'], axis=1, inplace=True)\n",
        "    trainLabels = trainLabels.dropna()\n",
        "    print(trainLabels.shape)\n",
        "    print(\"Writing CSV\")\n",
        "    trainLabels.to_csv('../labels/trainLabels_master_256_v2.csv', index=False, header=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2.5. Saving images as numpy array\n",
        "\n",
        "In this step, \n",
        "- we first append the suffix \".jpeg\" for all images in the dataframe\n",
        "- save the data object as a numpy file, used for saving train and test arrays"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2eZoHGGlmx7"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "def change_image_name(df, column):\n",
        "    return [i + '.jpeg' for i in df[column]]\n",
        "\n",
        "\n",
        "def convert_images_to_arrays_train(file_path, df):\n",
        "    lst_imgs = [l for l in df['train_image_name']]\n",
        "    return np.array([np.array(Image.open(file_path + img)) for img in lst_imgs])\n",
        "\n",
        "\n",
        "def save_to_array(arr_name, arr_object):\n",
        "    return np.save(arr_name, arr_object)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    start_time = time.time()\n",
        "    labels = pd.read_csv(\"../labels/trainLabels_master_256_v2.csv\")\n",
        "    print(\"Writing Train Array\")\n",
        "    X_train = convert_images_to_arrays_train('../data/train-resized-256/', labels)\n",
        "    print(X_train.shape)\n",
        "    print(\"Saving Train Array\")\n",
        "    save_to_array('../data/X_train.npy', X_train)\n",
        "    print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Convolutional Neural Network\n",
        "\n",
        "Functions involed:\n",
        "- Splitting the data into test and training datasets (X_train, X_test, y_train, y_test)\n",
        "- Reshaping the data into the format for CNN\n",
        "- Defining the CNN model using Sequential\n",
        "- Saving the model as a .h5 file to be able to use it for live prediction in the web"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import TensorBoard\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.models import Sequential\n",
        "from keras.utils import np_utils\n",
        "from keras.utils import multi_gpu_model\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "np.random.seed(1337)\n",
        "\n",
        "\n",
        "def split_data(X, y, test_data_size):\n",
        "    return train_test_split(X, y, test_size=test_data_size, random_state=42)\n",
        "\n",
        "\n",
        "def reshape_data(arr, img_rows, img_cols, channels):\n",
        "    return arr.reshape(arr.shape[0], img_rows, img_cols, channels)\n",
        "\n",
        "\n",
        "def cnn_model(X_train, y_train, kernel_size, nb_filters, channels, nb_epoch, batch_size, nb_classes, nb_gpus):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(nb_filters, (kernel_size[0], kernel_size[1]),\n",
        "                     padding='valid',\n",
        "                     strides=1,\n",
        "                     input_shape=(img_rows, img_cols, channels), activation=\"relu\"))\n",
        "    model.add(Conv2D(nb_filters, (kernel_size[0], kernel_size[1]), activation=\"relu\"))\n",
        "    model.add(Conv2D(nb_filters, (kernel_size[0], kernel_size[1]), activation=\"relu\"))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Flatten())\n",
        "    print(\"Model flattened out to: \", model.output_shape)\n",
        "    model.add(Dense(128))\n",
        "    model.add(Activation('sigmoid'))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Dense(nb_classes))\n",
        "    model.add(Activation('softmax'))\n",
        "    model = multi_gpu_model(model, gpus=nb_gpus)\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "    stop = EarlyStopping(monitor='val_acc',\n",
        "                         min_delta=0.001,\n",
        "                         patience=2,\n",
        "                         verbose=0,\n",
        "                         mode='auto')\n",
        "    tensor_board = TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
        "    model.fit(X_train, y_train, batch_size=batch_size, epochs=nb_epoch,\n",
        "              verbose=1,\n",
        "              validation_split=0.2,\n",
        "              class_weight='auto',\n",
        "              callbacks=[stop, tensor_board])\n",
        "    return model\n",
        "\n",
        "\n",
        "def save_model(model, score, model_name):\n",
        "    if score >= 0.75:\n",
        "        print(\"Saving Model\")\n",
        "        model.save(\"../models/\" + model_name + \"_recall_\" + str(round(score, 4)) + \".h5\")\n",
        "    else:\n",
        "        print(\"Model Not Saved.  Score: \", score)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Specify parameters before model is run.\n",
        "    batch_size = 512\n",
        "    nb_classes = 2\n",
        "    nb_epoch = 30\n",
        "\n",
        "    img_rows, img_cols = 256, 256\n",
        "    channels = 3\n",
        "    nb_filters = 32\n",
        "    kernel_size = (8, 8)\n",
        "\n",
        "    # Import data\n",
        "    labels = pd.read_csv(\"../labels/trainLabels_master_256_v2.csv\")\n",
        "    X = np.load(\"../data/X_train_256_v2.npy\")\n",
        "    y = np.array([1 if l >= 1 else 0 for l in labels['level']])\n",
        "\n",
        "    print(\"Splitting data into test/ train datasets\")\n",
        "    X_train, X_test, y_train, y_test = split_data(X, y, 0.2)\n",
        "\n",
        "    print(\"Reshaping Data\")\n",
        "    X_train = reshape_data(X_train, img_rows, img_cols, channels)\n",
        "    X_test = reshape_data(X_test, img_rows, img_cols, channels)\n",
        "\n",
        "    print(\"X_train Shape: \", X_train.shape)\n",
        "    print(\"X_test Shape: \", X_test.shape)\n",
        "\n",
        "    input_shape = (img_rows, img_cols, channels)\n",
        "\n",
        "    print(\"Normalizing Data\")\n",
        "    X_train = X_train.astype('float32')\n",
        "    X_test = X_test.astype('float32')\n",
        "\n",
        "    X_train /= 255\n",
        "    X_test /= 255\n",
        "\n",
        "    y_train = np_utils.to_categorical(y_train, nb_classes)\n",
        "    y_test = np_utils.to_categorical(y_test, nb_classes)\n",
        "    print(\"y_train Shape: \", y_train.shape)\n",
        "    print(\"y_test Shape: \", y_test.shape)\n",
        "\n",
        "    print(\"Training Model\")\n",
        "\n",
        "    model = cnn_model(X_train, y_train, kernel_size, nb_filters, channels, nb_epoch, batch_size,\n",
        "                      nb_classes, nb_gpus=8)\n",
        "\n",
        "    print(\"Predicting\")\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    score = model.evaluate(X_test, y_test, verbose=0)\n",
        "    print('Test score:', score[0])\n",
        "    print('Test accuracy:', score[1])\n",
        "\n",
        "    y_test = np.argmax(y_test, axis=1)\n",
        "    y_pred = np.argmax(y_pred, axis=1)\n",
        "\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "\n",
        "    print(\"Precision: \", precision)\n",
        "    print(\"Recall: \", recall)\n",
        "\n",
        "    save_model(model=model, score=recall, model_name=\"DR_Two_Classes\")\n",
        "    print(\"Completed\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
